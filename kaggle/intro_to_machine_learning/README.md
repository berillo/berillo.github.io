# Kaggle: Intro to Machine Learning

This folder contains my solutions and notes from the [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) course on Kaggle.

## What This Course Covers

The course introduces core concepts of supervised learning using the **Iowa housing dataset**, where the goal is to predict home sale prices.
- Building simple models (Decision Trees)  
- Understanding underfitting vs. overfitting  
- Splitting data into training and validation sets  
- Improving accuracy with Random Forests  
- Using scikit-learn in Python

## My Exercises
- **[Exercise 1 – Model Building](exercise1.ipynb)**  
  Built my first decision tree to predict house prices in Iowa.  

- **[Exercise 2 – Model Validation](exercise2.ipynb)**  
  Learned how to split data into training/validation sets and check accuracy. 

- **[Exercise 3 – Random Forests](exercise3.ipynb)**  
  Improved model performance using Random Forests instead of a single tree.

## Key Takeaways

- Good performance on training data doesn’t guarantee good predictions on new data → validation is essential.  
- Random Forests usually outperform a single tree by reducing overfitting.  
- Data features (like lot size, year built, etc.) are crucial for model accuracy.


