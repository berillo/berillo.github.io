WEEK 2 â€” REGRESSION WITH MULTIPLE INPUT VARIABLES
==================================================
This week extends Linear Regression to multiple input variables (features)
and introduces Vectorization â€” a technique that speeds up computations
by performing many operations simultaneously. 

I also explore how Gradient Descent works with multiple features, how to scale features for faster
convergence, and how to enhance models using Feature Engineering and
Polynomial Regression.

LEARNING OBJECTIVES
-------------------
By the end of Week 2, I will be able to:

* Represent a multiple linear regression model mathematically.
* Implement vectorized operations to make training efficient.
* Apply gradient descent for multivariate regression.
* Use feature scaling to improve convergence speed and stability.
* Understand feature engineering and polynomial regression for complex data.
* Train linear regression models using scikit-learn.

TOPICS COVERED
--------------

ðŸ”¹ Multiple Linear Regression
* Regression with Multiple Input Variables
* Understanding multiple features and their representation
* Matrix form of the hypothesis
* âœ… Optional Lab: Multiple Linear Regression

ðŸ”¹ Vectorization
* Vectorization
* Performing efficient operations using NumPy arrays
* âœ… Optional Lab: Python, NumPy and Vectorization

ðŸ”¹ Gradient Descent for Multiple Linear Regression
* Gradient Descent for Multiple Linear Regression
* Cost Function
* Gradient Descent (vectorized)
* âœ… Practice Quiz: Multiple Linear Regression â€” Score 100%

ðŸ”¹ Feature Scaling and Learning Rate
* Feature Scaling
* Checking Gradient Descent for Convergence
* Choosing the Learning Rate
* Feature Normalization Formula
* ðŸ”œ Optional Lab: Feature Scaling and Learning Rate

ðŸ”¹ Feature Engineering and Polynomial Regression
* Feature Engineering
* Polynomial Regression
* Expanding features to capture non-linear relationships
* ðŸ”œ Optional Lab: Feature Engineering and Polynomial Regression

ðŸ”¹ Linear Regression with Scikit-Learn
* Implementing linear regression using scikit-learn
* Comparing manual and library-based implementations
* ðŸ”œ Optional Lab: Linear Regression with scikit-learn

ðŸ”¹ Gradient Descent in Practice
* Understanding convergence and learning rate tuning
* âœ… Practice Quiz: Gradient Descent in Practice 100%

ðŸ”¹ Week 2 Practice Lab
* Integrating multiple features, scaling, and optimization
* ðŸ”œ Programming Assignment: Week 2 Practice Lab â€” Linear Regression

Labs & Quizzes Summary
----------------------
Type    | Title                                           | Status
--------|-------------------------------------------------|---------
Lab     | Python, NumPy and Vectorization                 | âœ… Completed
Lab     | Multiple Linear Regression                      | âœ… Completed
Lab     | Feature Scaling and Learning Rate               | ðŸ”œ To be completed
Lab     | Feature Engineering and Polynomial Regression   | ðŸ”œ To be completed
Lab     | Linear Regression with Scikit-Learn             | ðŸ”œ To be completed
Quiz    | Multiple Linear Regression                      | âœ… 100%
Quiz    | Gradient Descent in Practice                    | âœ… 100%
Project | Week 2 Practice Lab â€“ Linear Regression         | ðŸ”œ To be completed

Folder Structure
----------------
* README.txt  â†’ Main summary for Week 2 (topics, progress, quizzes, etc.)
* notebooks/  â†’ Contains all Coursera lab notebooks for this week.
* notes/      â†’ My own handwritten-style explanations and formulas.
* data/       â†’ Any datasets used for the labs.
* images/     â†’ Visuals included in the README or notes.
* scripts/    â†’ Python versions of key notebook code blocks (for re-use).
* requirements.txt â†’ List of Python packages required to run notebooks locally.

